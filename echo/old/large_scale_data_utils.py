# fixed_large_scale_data_utils.py - ãƒ‡ãƒ¼ãƒ¢ãƒ³ãƒ—ãƒ­ã‚»ã‚¹å•é¡Œä¿®æ­£ç‰ˆ

import os
import sqlite3
import numpy as np
import torch
import pickle
import lz4.frame
import hashlib
import threading
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import psutil
import cv2
import pydicom as dicom
from torch.utils.data import Dataset
import pandas as pd
import random
from collections import defaultdict, deque
import gc
import logging

class FixedDICOMProcessor:
    """ä¿®æ­£ç‰ˆDICOMå‡¦ç†å™¨ï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å•é¡Œã‚’å›é¿ï¼‰"""
    
    def __init__(self, cache_dir, max_memory_cache_gb=8, max_disk_cache_gb=100):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ¡ãƒ¢ãƒªåˆ¶é™
        self.max_memory_cache = max_memory_cache_gb * 1024**3
        self.max_disk_cache = max_disk_cache_gb * 1024**3
        self.current_memory_usage = 0
        
        # SQLiteãƒ™ãƒ¼ã‚¹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†
        self.db_path = self.cache_dir / "metadata.db"
        self.init_database()
        
        # éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.memory_cache = {}  # Hot data
        self.compressed_cache = {}  # Warm data (compressed)
        self.access_history = deque(maxlen=5000)  # LRU tracking
        self.cache_lock = threading.RLock()
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã®ã¿ä½¿ç”¨ï¼ˆãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«å›é¿ï¼‰
        self.io_pool = ThreadPoolExecutor(max_workers=4, thread_name_prefix="dicom_io")
        
        # çµ±è¨ˆ
        self.stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'disk_reads': 0,
            'compressions': 0,
            'decompressions': 0,
            'processing_errors': 0
        }
        
        print(f"ğŸ—ï¸  ä¿®æ­£ç‰ˆDICOMå‡¦ç†å™¨åˆæœŸåŒ–")
        print(f"   - ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸Šé™: {max_memory_cache_gb}GB")
        print(f"   - ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸Šé™: {max_disk_cache_gb}GB")
        print(f"   - I/Oã‚¹ãƒ¬ãƒƒãƒ‰: {self.io_pool._max_workers}")
    
    def init_database(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨SQLite DBåˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS dicom_metadata (
                    file_path TEXT PRIMARY KEY,
                    cache_key TEXT,
                    file_size INTEGER,
                    last_modified REAL,
                    last_accessed REAL,
                    compressed_size INTEGER,
                    processing_time REAL,
                    access_count INTEGER DEFAULT 0
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_last_accessed ON dicom_metadata(last_accessed)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_access_count ON dicom_metadata(access_count)")
            conn.commit()
    
    def get_cache_key(self, dicom_path, params=None):
        """é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        if params is None:
            params = {'n_frames': 32, 'frame_stride': 2, 'video_size': 224}
        
        try:
            stat = os.stat(dicom_path)
            file_info = f"{dicom_path}_{stat.st_size}_{stat.st_mtime_ns}"
        except OSError:
            file_info = str(dicom_path)
        
        param_str = "_".join(f"{k}{v}" for k, v in sorted(params.items()))
        cache_string = f"{file_info}_{param_str}"
        
        return hashlib.blake2b(cache_string.encode(), digest_size=16).hexdigest()
    
    def process_dicom_single(self, dicom_path, params=None):
        """å˜ä¸€DICOMå‡¦ç†ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¾‹å¤–ã‚’ä¸Šã’ã‚‹ï¼‰"""
        if params is None:
            params = {'n_frames': 32, 'frame_stride': 2, 'video_size': 224}
        
        cache_key = self.get_cache_key(dicom_path, params)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cached_data = self._get_from_cache(cache_key, dicom_path)
        if cached_data is not None:
            self.stats['cache_hits'] += 1
            return cached_data
        
        # æ–°è¦å‡¦ç†
        self.stats['cache_misses'] += 1
        start_time = time.time()
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¾‹å¤–ã‚’ä¸Šã’ã‚‹ï¼ˆã‚¼ãƒ­ãƒ†ãƒ³ã‚½ãƒ«è¿”å´ã‚’å‰Šé™¤ï¼‰
        result = self._process_dicom_optimized(dicom_path, params)
        processing_time = time.time() - start_time
        
        # éåŒæœŸã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        self.io_pool.submit(self._save_to_cache, cache_key, dicom_path, result, processing_time)
        
        return result
    
    def process_dicom_batch_sequential(self, dicom_paths, params=None):
        """ãƒãƒƒãƒDICOMå‡¦ç†ï¼ˆé †æ¬¡å‡¦ç†ç‰ˆï¼‰"""
        results = {}
        
        for path in dicom_paths:
            try:
                result = self.process_dicom_single(path, params)
                results[path] = result
            except Exception as e:
                print(f"âš ï¸ ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ {path}: {e}")
                if params is None:
                    params = {'n_frames': 32, 'frame_stride': 2, 'video_size': 224}
                results[path] = torch.zeros(3, params['n_frames']//params['frame_stride'], 
                                         params['video_size'], params['video_size'])
        
        return results
    
    def _get_from_cache(self, cache_key, file_path):
        """éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        current_time = time.time()
        
        with self.cache_lock:
            # 1. ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
            if cache_key in self.memory_cache:
                self.access_history.append((cache_key, current_time))
                self._update_access_record_async(file_path)
                return self.memory_cache[cache_key].clone()
            
            # 2. åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
            if cache_key in self.compressed_cache:
                try:
                    compressed_data = self.compressed_cache[cache_key]
                    decompressed = lz4.frame.decompress(compressed_data)
                    tensor_data = pickle.loads(decompressed)
                    
                    # ãƒ›ãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ˜‡æ ¼
                    self._manage_memory_cache(tensor_data.nbytes)
                    self.memory_cache[cache_key] = tensor_data.clone()
                    
                    self.stats['decompressions'] += 1
                    return tensor_data
                except Exception as e:
                    print(f"åœ§ç¸®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
                    del self.compressed_cache[cache_key]
        
        # 3. ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        cache_file = self.cache_dir / f"{cache_key}.lz4"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    compressed_data = f.read()
                
                decompressed = lz4.frame.decompress(compressed_data)
                tensor_data = pickle.loads(decompressed)
                
                with self.cache_lock:
                    # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«èª­ã¿è¾¼ã¿
                    self._manage_memory_cache(tensor_data.nbytes)
                    self.memory_cache[cache_key] = tensor_data.clone()
                
                self.stats['disk_reads'] += 1
                self._update_access_record_async(file_path)
                return tensor_data
            except Exception as e:
                print(f"ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                cache_file.unlink(missing_ok=True)
        
        return None
    
    def _save_to_cache(self, cache_key, file_path, tensor_data, processing_time=0):
        """éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        try:
            with self.cache_lock:
                data_size = tensor_data.nbytes
                
                # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self._manage_memory_cache(data_size)
                self.memory_cache[cache_key] = tensor_data.clone()
                self.current_memory_usage += data_size
            
            # åœ§ç¸®ã—ã¦ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜
            pickled_data = pickle.dumps(tensor_data.cpu(), protocol=pickle.HIGHEST_PROTOCOL)
            compressed_data = lz4.frame.compress(pickled_data, compression_level=1)
            
            cache_file = self.cache_dir / f"{cache_key}.lz4"
            with open(cache_file, 'wb') as f:
                f.write(compressed_data)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            self._update_metadata(file_path, cache_key, len(compressed_data), processing_time)
            
            self.stats['compressions'] += 1
            
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _manage_memory_cache(self, new_data_size):
        """ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ï¼ˆLRUï¼‰"""
        while (self.current_memory_usage + new_data_size > self.max_memory_cache 
               and self.memory_cache):
            
            # æœ€ã‚‚å¤ã„ã‚¢ã‚¯ã‚»ã‚¹ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤
            if self.access_history:
                # ã‚¢ã‚¯ã‚»ã‚¹å±¥æ­´ã‹ã‚‰å‰Šé™¤å€™è£œé¸æŠ
                for cache_key, _ in list(self.access_history)[:50]:
                    if cache_key in self.memory_cache:
                        tensor_data = self.memory_cache.pop(cache_key)
                        self.current_memory_usage -= tensor_data.nbytes
                        
                        # åœ§ç¸®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«é™æ ¼
                        try:
                            pickled = pickle.dumps(tensor_data.cpu(), protocol=pickle.HIGHEST_PROTOCOL)
                            compressed = lz4.frame.compress(pickled, compression_level=1)
                            self.compressed_cache[cache_key] = compressed
                        except Exception:
                            pass  # åœ§ç¸®å¤±æ•—æ™‚ã¯ç ´æ£„
                        
                        if self.current_memory_usage + new_data_size <= self.max_memory_cache:
                            break
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤
                oldest_key = next(iter(self.memory_cache))
                removed_data = self.memory_cache.pop(oldest_key)
                self.current_memory_usage -= removed_data.nbytes
    
    def _process_dicom_optimized(self, dicom_path, params):
        """æœ€é©åŒ–ã•ã‚ŒãŸDICOMå‡¦ç†ï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°çµ±ä¸€ï¼‰"""
        n_frames = params['n_frames']
        frame_stride = params['frame_stride'] 
        video_size = params['video_size']
        target_frames = n_frames // frame_stride  # æœ€çµ‚çš„ãªç›®æ¨™ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
        
        # é«˜é€ŸDICOMèª­ã¿è¾¼ã¿
        dcm = dicom.dcmread(dicom_path, defer_size="1KB")
        pixels = dcm.pixel_array
        
        if pixels is None or pixels.size == 0:
            raise ValueError(f"DICOMãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {dicom_path}")
        
        # åŠ¹ç‡çš„ãªå‰å‡¦ç†
        if pixels.ndim == 3:
            pixels = np.repeat(pixels[..., None], 3, axis=3)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°åˆ¶é™ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ï¼‰
        if pixels.shape[0] > n_frames * 2:
            step = max(1, pixels.shape[0] // n_frames)
            pixels = pixels[::step][:n_frames]
        
        # ç°¡ç•¥ãƒã‚¹ã‚­ãƒ³ã‚°
        if len(pixels) >= 2:
            try:
                first = cv2.cvtColor(pixels[0].astype(np.uint8), cv2.COLOR_RGB2GRAY)
                last = cv2.cvtColor(pixels[-1].astype(np.uint8), cv2.COLOR_RGB2GRAY)
                diff = cv2.absdiff(first, last)
                _, mask = cv2.threshold(diff, 15, 255, cv2.THRESH_BINARY)
                
                # å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒã‚¹ã‚¯é©ç”¨
                for i in range(len(pixels)):
                    if pixels[i].ndim == 3:
                        pixels[i] = cv2.bitwise_and(pixels[i].astype(np.uint8), 
                                                  pixels[i].astype(np.uint8), mask=mask)
            except Exception:
                pass  # ãƒã‚¹ã‚­ãƒ³ã‚°å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
        
        # ãƒªã‚µã‚¤ã‚ºå‡¦ç†
        processed_frames = []
        for frame in pixels:
            resized = cv2.resize(frame, (video_size, video_size), interpolation=cv2.INTER_LINEAR)
            processed_frames.append(resized)
        
        if not processed_frames:
            raise ValueError(f"å‡¦ç†å¯èƒ½ãªãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“: {dicom_path}")
        
        # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›
        x = torch.tensor(np.array(processed_frames), dtype=torch.float32).permute(3, 0, 1, 2)
        
        # æ­£è¦åŒ–ï¼ˆEchoPrimeçµ±è¨ˆï¼‰
        mean = torch.tensor([29.110628, 28.076836, 29.096405]).view(3, 1, 1, 1)
        std = torch.tensor([47.989223, 46.456997, 47.20083]).view(3, 1, 1, 1)
        x = (x - mean) / std
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’ç›®æ¨™æ•°ã«çµ±ä¸€ï¼ˆé‡è¦ãªä¿®æ­£ï¼‰
        current_frames = x.shape[1]
        
        if current_frames < target_frames:
            # ä¸è¶³åˆ†ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            padding = torch.zeros(3, target_frames - current_frames, video_size, video_size)
            x = torch.cat([x, padding], dim=1)
        elif current_frames > target_frames:
            # ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ã¾ãŸã¯åˆ‡ã‚Šå–ã‚Š
            if current_frames >= target_frames * frame_stride:
                x = x[:, ::frame_stride][:, :target_frames]
            else:
                x = x[:, :target_frames]
        
        # æœ€çµ‚ç¢ºèªï¼šå¿…ãš target_frames ã«ãªã‚‹ã‚ˆã†ã«ã™ã‚‹
        if x.shape[1] != target_frames:
            if x.shape[1] < target_frames:
                padding = torch.zeros(3, target_frames - x.shape[1], video_size, video_size)
                x = torch.cat([x, padding], dim=1)
            else:
                x = x[:, :target_frames]
        
        return x
    
    def _update_metadata(self, file_path, cache_key, compressed_size, processing_time=0):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                stat = os.stat(file_path)
                current_time = time.time()
                
                conn.execute("""
                    INSERT OR REPLACE INTO dicom_metadata 
                    (file_path, cache_key, file_size, last_modified, last_accessed, 
                     compressed_size, processing_time, access_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?, 
                            COALESCE((SELECT access_count FROM dicom_metadata WHERE file_path = ?), 0) + 1)
                """, (file_path, cache_key, stat.st_size, stat.st_mtime, 
                      current_time, compressed_size, processing_time, file_path))
                conn.commit()
        except Exception as e:
            print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _update_access_record_async(self, file_path):
        """ã‚¢ã‚¯ã‚»ã‚¹è¨˜éŒ²æ›´æ–°ï¼ˆéåŒæœŸï¼‰"""
        def update_db():
            try:
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("""
                        UPDATE dicom_metadata 
                        SET last_accessed = ?, access_count = access_count + 1
                        WHERE file_path = ?
                    """, (time.time(), file_path))
                    conn.commit()
            except Exception:
                pass
        
        self.io_pool.submit(update_db)
    
    def get_stats(self):
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        with self.cache_lock:
            memory_usage_mb = self.current_memory_usage / 1e6
            
        try:
            disk_usage_gb = sum(f.stat().st_size for f in self.cache_dir.glob("*.lz4")) / 1e9
        except Exception:
            disk_usage_gb = 0
            
        hit_rate = self.stats['cache_hits'] / max(self.stats['cache_hits'] + self.stats['cache_misses'], 1)
        
        return {
            'memory_cache_items': len(self.memory_cache),
            'compressed_cache_items': len(self.compressed_cache),
            'memory_usage_mb': memory_usage_mb,
            'disk_usage_gb': disk_usage_gb,
            'cache_hit_rate': hit_rate,
            **self.stats
        }
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        with self.cache_lock:
            self.memory_cache.clear()
            self.compressed_cache.clear()
            self.current_memory_usage = 0
            self.access_history.clear()
        gc.collect()
    
    def shutdown(self):
        """ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾"""
        self.io_pool.shutdown(wait=True)


class SimplifiedEchoDataset(Dataset):
    """ç°¡ç•¥åŒ–ã•ã‚ŒãŸEchoãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å•é¡Œå›é¿ï¼‰"""
    
    def __init__(self, csv_path, config, dicom_processor, 
                 max_samples_per_epoch=2000, is_validation=False, seed=42):
        self.csv_path = csv_path
        self.config = config
        self.dicom_processor = dicom_processor
        self.max_samples_per_epoch = max_samples_per_epoch
        self.is_validation = is_validation
        self.seed = seed
        self.epoch = 0
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.df = pd.read_csv(csv_path)
        self.total_samples = len(self.df)
        
        # ä¿è­·å±æ€§æº–å‚™
        self.protected_attrs = self._prepare_protected_attributes()
        
        # åˆæœŸãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç”Ÿæˆ
        self.regenerate_triplets()
        
        print(f"ğŸ“Š ç°¡ç•¥åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
        print(f"   - ç·ã‚µãƒ³ãƒ—ãƒ«: {self.total_samples:,}")
        print(f"   - ã‚¨ãƒãƒƒã‚¯æ¯æœ€å¤§ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆ: {max_samples_per_epoch:,}")
        print(f"   - ç¾åœ¨ã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆæ•°: {len(self.triplets):,}")
    
    def _prepare_protected_attributes(self):
        """ä¿è­·å±æ€§æº–å‚™"""
        protected_attrs = {}
        sex_mapping = {'M': 0, 'F': 1}
        race_mapping = {'White': 0, 'Black': 1, 'Hispanic': 2, 'Asian': 2, 'Unknown': 2}
        
        for _, row in self.df.iterrows():
            protected_attrs[row['dicom_path']] = {
                'Sex': sex_mapping.get(row['Sex'], 0),
                'Race': race_mapping.get(row['Race'], 0),
            }
        return protected_attrs
    
    def set_epoch(self, epoch):
        """ã‚¨ãƒãƒƒã‚¯è¨­å®š"""
        self.epoch = epoch
        self.regenerate_triplets()
    
    def regenerate_triplets(self):
        """ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆå†ç”Ÿæˆ"""
        print(f"ğŸ”„ ã‚¨ãƒãƒƒã‚¯{self.epoch}: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç”Ÿæˆä¸­...")
        
        # ã‚·ãƒ¼ãƒ‰è¨­å®š
        random.seed(self.seed + self.epoch * 1000)
        np.random.seed(self.seed + self.epoch * 1000)
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
        if not self.is_validation and len(self.df) > self.max_samples_per_epoch:
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sampled_df = self.df.sample(n=self.max_samples_per_epoch, random_state=self.seed + self.epoch)
        else:
            sampled_df = self.df.copy()
        
        # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç”Ÿæˆ
        self.triplets = self._generate_triplets(sampled_df)
        
        print(f"   ç”Ÿæˆå®Œäº†: {len(self.triplets)}å€‹ã®ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆ")
    
    def _generate_triplets(self, df):
        """åŠ¹ç‡çš„ãªãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç”Ÿæˆ"""
        triplets = []
        
        # viewåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        view_groups = df.groupby('view')
        
        for view, view_df in view_groups:
            # subject_idåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            subject_groups = view_df.groupby('subject_id')
            
            subject_ids = list(subject_groups.groups.keys())
            if len(subject_ids) < 2:
                continue
            
            # å„è¢«é¨“è€…ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
            subject_samples = {sid: group.to_dict('records') 
                             for sid, group in subject_groups}
            
            # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç”Ÿæˆ
            for subject_id in subject_ids:
                samples = subject_samples[subject_id]
                if len(samples) < 2:
                    continue
                
                # ä»–ã®è¢«é¨“è€…ã®ã‚µãƒ³ãƒ—ãƒ«
                other_samples = []
                for other_sid in subject_ids:
                    if other_sid != subject_id:
                        other_samples.extend(subject_samples[other_sid])
                
                if not other_samples:
                    continue
                
                # å„ã‚µãƒ³ãƒ—ãƒ«ã‚’anchorã¨ã—ã¦ä½¿ç”¨
                max_triplets_per_anchor = 1 if self.is_validation else 2
                
                for anchor_sample in samples:
                    pos_candidates = [s for s in samples if s['dicom_path'] != anchor_sample['dicom_path']]
                    if not pos_candidates:
                        continue
                    
                    # ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç”Ÿæˆï¼ˆåˆ¶é™ä»˜ãï¼‰
                    for _ in range(min(max_triplets_per_anchor, len(pos_candidates), len(other_samples))):
                        pos_sample = random.choice(pos_candidates)
                        neg_sample = random.choice(other_samples)
                        
                        triplet = {
                            'anchor_path': anchor_sample['dicom_path'],
                            'positive_path': pos_sample['dicom_path'],
                            'negative_path': neg_sample['dicom_path'],
                        }
                        
                        # Adversarialå±æ€§
                        if self.config and self.config.use_adversarial:
                            for attr in self.config.adversarial_attributes:
                                attr_value = self.protected_attrs.get(anchor_sample['dicom_path'], {}).get(attr, 0)
                                triplet[f'{attr}_value'] = attr_value
                        
                        triplets.append(triplet)
        
        random.shuffle(triplets)
        return triplets
    
    def __len__(self):
        return len(self.triplets)
    
    def __getitem__(self, idx):
        """ã‚¢ã‚¤ãƒ†ãƒ å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’è©¦è¡Œï¼‰"""
        max_retries = 10  # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        
        for retry in range(max_retries):
            try:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª¿æ•´
                current_idx = (idx + retry) % len(self.triplets)
                triplet = self.triplets[current_idx]
                
                # DICOMå‡¦ç†
                anchor = self.dicom_processor.process_dicom_single(triplet['anchor_path'])
                positive = self.dicom_processor.process_dicom_single(triplet['positive_path'])
                negative = self.dicom_processor.process_dicom_single(triplet['negative_path'])
                
                # ã‚¼ãƒ­ãƒ†ãƒ³ã‚½ãƒ«ãƒã‚§ãƒƒã‚¯ï¼ˆå‡¦ç†å¤±æ•—ã®æ¤œå‡ºï¼‰
                if (anchor.sum() == 0 or positive.sum() == 0 or negative.sum() == 0):
                    if retry < max_retries - 1:
                        continue  # æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’è©¦è¡Œ
                    else:
                        raise ValueError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
                # ã‚µãƒ³ãƒ—ãƒ«ä½œæˆ
                sample = {
                    'anchor': anchor,
                    'positive': positive,
                    'negative': negative
                }
                
                # Adversarialå±æ€§
                if self.config and self.config.use_adversarial:
                    for attr in self.config.adversarial_attributes:
                        attr_value = triplet.get(f'{attr}_value', 0)
                        sample[attr] = torch.tensor(attr_value, dtype=torch.long)
                
                return sample
                
            except Exception as e:
                if retry < max_retries - 1:
                    print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ (retry {retry+1}/{max_retries}): {e}")
                    continue  # æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’è©¦è¡Œ
                else:
                    print(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {e}")
                    # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦æ¬¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã™
                    return self.__getitem__((idx + max_retries) % len(self.triplets))


def create_fixed_dataloader(csv_path, config, cache_dir, 
                           max_memory_cache_gb=8, max_disk_cache_gb=100,
                           max_samples_per_epoch=2000, batch_size=4, 
                           num_workers=0, is_validation=False, **kwargs):
    """ä¿®æ­£ç‰ˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å•é¡Œå›é¿ï¼‰"""
    
    # DICOMå‡¦ç†å™¨
    dicom_processor = FixedDICOMProcessor(
        cache_dir=cache_dir,
        max_memory_cache_gb=max_memory_cache_gb,
        max_disk_cache_gb=max_disk_cache_gb
    )
    
    # ç°¡ç•¥åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    dataset = SimplifiedEchoDataset(
        csv_path=csv_path,
        config=config,
        dicom_processor=dicom_processor,
        max_samples_per_epoch=max_samples_per_epoch,
        is_validation=is_validation,
        **kwargs
    )
    
    # DataLoaderï¼ˆnum_workers=0ã§ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å›é¿ï¼‰
    from torch.utils.data import DataLoader
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=not is_validation,
        num_workers=num_workers,  # é€šå¸¸ã¯0ã‚’æ¨å¥¨
        pin_memory=True if torch.cuda.is_available() else False,
        drop_last=not is_validation
    )
    
    return dataloader, dataset, dicom_processor