# video_loading_diagnosis.py - ãƒ“ãƒ‡ã‚ªãƒ­ãƒ¼ãƒ‰å•é¡Œã®è©³ç´°è¨ºæ–­

import pandas as pd
import cv2
import os
import time
import torch
import numpy as np
from pathlib import Path
import concurrent.futures
from collections import defaultdict

class VideoLoadingDiagnostic:
    """ãƒ“ãƒ‡ã‚ªãƒ­ãƒ¼ãƒ‰å•é¡Œã®è©³ç´°è¨ºæ–­"""
    
    def __init__(self, csv_path):
        self.csv_path = csv_path
        self.df = pd.read_csv(csv_path)
        print(f"ğŸ“Š Loaded dataset: {len(self.df)} samples")
        
    def analyze_file_system(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ‘ã‚¹åˆ†æ"""
        print("\nğŸ—‚ï¸  FILE SYSTEM ANALYSIS")
        print("="*50)
        
        # ãƒ‘ã‚¹åˆ†æ
        sample_paths = []
        for col in ['dicom_path']:
            if col in self.df.columns:
                sample_paths.extend(self.df[col].dropna().head(10).tolist())
        
        for i, path in enumerate(sample_paths[:5]):
            print(f"Sample path {i+1}: {path}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
            exists = os.path.exists(path)
            
            if exists:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
                size_mb = os.path.getsize(path) / (1024*1024)
                
                # ãƒ‡ã‚£ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—æ¨å®šï¼ˆç°¡æ˜“ï¼‰
                disk_type = "Unknown"
                if "/mnt/" in path:
                    disk_type = "Mounted drive (possibly network/external)"
                elif "SSD" in path.upper() or "NVME" in path.upper():
                    disk_type = "SSD (estimated)"
                elif "HDD" in path.upper():
                    disk_type = "HDD (estimated)"
                
                print(f"  âœ… Exists: {size_mb:.1f}MB ({disk_type})")
            else:
                print(f"  âŒ Missing file")
        
        # ãƒ‘ã‚¹ã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³
        common_dirs = defaultdict(int)
        for path in sample_paths:
            parent_dir = str(Path(path).parent)
            common_dirs[parent_dir] += 1
        
        print(f"\nCommon directories:")
        for dir_path, count in sorted(common_dirs.items(), key=lambda x: x[1], reverse=True)[:3]:
            print(f"  {dir_path}: {count} files")
    
    def benchmark_single_video_loading(self, num_videos=10):
        """å˜ä¸€ãƒ“ãƒ‡ã‚ªèª­ã¿è¾¼ã¿ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print(f"\nğŸ“¹ SINGLE VIDEO LOADING BENCHMARK")
        print("="*50)
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ“ãƒ‡ã‚ªãƒ‘ã‚¹ã‚’å–å¾—
        test_paths = []
        for col in ['dicom_path']:
            if col in self.df.columns:
                test_paths.extend(self.df[col].dropna().head(num_videos//3).tolist())
        
        test_paths = test_paths[:num_videos]
        
        load_times = []
        file_sizes = []
        error_count = 0
        
        for i, video_path in enumerate(test_paths):
            print(f"\nTesting video {i+1}: {os.path.basename(video_path)}")
            
            if not os.path.exists(video_path):
                print(f"  âŒ File not found")
                error_count += 1
                continue
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
            file_size_mb = os.path.getsize(video_path) / (1024*1024)
            file_sizes.append(file_size_mb)
            print(f"  ğŸ“ Size: {file_size_mb:.1f}MB")
            
            # ãƒ­ãƒ¼ãƒ‰æ™‚é–“æ¸¬å®š
            start_time = time.time()
            
            try:
                # OpenCVã§ãƒ“ãƒ‡ã‚ªèª­ã¿è¾¼ã¿
                cap = cv2.VideoCapture(video_path)
                
                if not cap.isOpened():
                    print(f"  âŒ Cannot open with OpenCV")
                    error_count += 1
                    continue
                
                # ãƒ“ãƒ‡ã‚ªæƒ…å ±å–å¾—
                fps = cap.get(cv2.CAP_PROP_FPS)
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                
                print(f"  ğŸ“Š Props: {width}x{height}, {frame_count} frames, {fps:.1f}fps")
                
                # 16ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
                frames = []
                frame_interval = max(1, frame_count // 16)
                
                for frame_idx in range(0, frame_count, frame_interval):
                    if len(frames) >= 16:
                        break
                    
                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                    ret, frame = cap.read()
                    
                    if ret:
                        # ãƒªã‚µã‚¤ã‚ºã¨æ­£è¦åŒ–ï¼ˆå®Ÿéš›ã®å‡¦ç†ï¼‰
                        frame = cv2.resize(frame, (224, 224))
                        frame = frame.astype(np.float32) / 255.0
                        frames.append(frame)
                
                cap.release()
                
                load_time = time.time() - start_time
                load_times.append(load_time)
                
                print(f"  â±ï¸  Load time: {load_time:.3f}s ({file_size_mb/load_time:.1f}MB/s)")
                
                # ç•°å¸¸ã«é…ã„å ´åˆã®è­¦å‘Š
                if load_time > 10:
                    print(f"  ğŸš¨ VERY SLOW: {load_time:.1f}s is abnormally slow!")
                elif load_time > 5:
                    print(f"  âš ï¸  SLOW: {load_time:.1f}s is slower than expected")
                elif load_time < 1:
                    print(f"  âœ… FAST: Good performance")
                
            except Exception as e:
                print(f"  âŒ Error: {e}")
                error_count += 1
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        if load_times:
            avg_load_time = np.mean(load_times)
            max_load_time = np.max(load_times)
            min_load_time = np.min(load_times)
            avg_file_size = np.mean(file_sizes) if file_sizes else 0
            
            print(f"\nğŸ“ˆ LOADING STATISTICS:")
            print(f"  Average load time: {avg_load_time:.3f}s")
            print(f"  Min/Max load time: {min_load_time:.3f}s / {max_load_time:.3f}s")
            print(f"  Average file size: {avg_file_size:.1f}MB")
            print(f"  Average throughput: {avg_file_size/avg_load_time:.1f}MB/s")
            print(f"  Error rate: {error_count}/{len(test_paths)} ({error_count/len(test_paths)*100:.1f}%)")
            
            # å•é¡Œè¨ºæ–­
            print(f"\nğŸ” DIAGNOSIS:")
            if avg_load_time > 5:
                print(f"  ğŸš¨ CRITICAL: Average load time ({avg_load_time:.1f}s) is extremely slow")
                print(f"     â†’ Likely causes: slow disk, network storage, large files")
            elif avg_load_time > 2:
                print(f"  âš ï¸  WARNING: Load time ({avg_load_time:.1f}s) is slower than optimal")
                print(f"     â†’ Consider: SSD upgrade, file compression, preprocessing")
            else:
                print(f"  âœ… Load time ({avg_load_time:.1f}s) seems reasonable")
            
            if max_load_time > avg_load_time * 3:
                print(f"  âš ï¸  Large variance: Some files are much slower than others")
                print(f"     â†’ Check: file corruption, different formats, network issues")
        
        return {
            'avg_load_time': np.mean(load_times) if load_times else float('inf'),
            'max_load_time': np.max(load_times) if load_times else float('inf'),
            'error_rate': error_count / len(test_paths) if test_paths else 1.0,
            'avg_file_size': np.mean(file_sizes) if file_sizes else 0
        }
    
    def test_parallel_loading(self, num_workers=4, num_videos=12):
        """ä¸¦åˆ—ãƒ­ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ"""
        print(f"\nğŸ”„ PARALLEL LOADING TEST ({num_workers} workers)")
        print("="*50)
        
        # ãƒ†ã‚¹ãƒˆç”¨ãƒ“ãƒ‡ã‚ªãƒ‘ã‚¹
        test_paths = []
        for col in ['dicom_path']:
            if col in self.df.columns:
                test_paths.extend(self.df[col].dropna().head(num_videos//3).tolist())
        
        test_paths = test_paths[:num_videos]
        existing_paths = [p for p in test_paths if os.path.exists(p)]
        
        if not existing_paths:
            print("âŒ No valid video files found for testing")
            return
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ­ãƒ¼ãƒ‰
        print("Testing sequential loading...")
        start_time = time.time()
        for path in existing_paths:
            self._load_video_simple(path)
        sequential_time = time.time() - start_time
        
        # ä¸¦åˆ—ãƒ­ãƒ¼ãƒ‰
        print(f"Testing parallel loading with {num_workers} workers...")
        start_time = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(self._load_video_simple, path) for path in existing_paths]
            concurrent.futures.wait(futures)
        parallel_time = time.time() - start_time
        
        # çµæœ
        speedup = sequential_time / parallel_time if parallel_time > 0 else 0
        
        print(f"\nğŸ“Š PARALLEL LOADING RESULTS:")
        print(f"  Sequential time: {sequential_time:.2f}s")
        print(f"  Parallel time: {parallel_time:.2f}s")
        print(f"  Speedup: {speedup:.2f}x")
        
        if speedup < 1.5:
            print(f"  âš ï¸  Poor parallelization - likely I/O bound")
            print(f"     â†’ Bottleneck: disk speed, not CPU")
        elif speedup > 2:
            print(f"  âœ… Good parallelization benefit")
        
        return speedup
    
    def _load_video_simple(self, video_path):
        """ç°¡å˜ãªãƒ“ãƒ‡ã‚ªãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
        try:
            cap = cv2.VideoCapture(video_path)
            if cap.isOpened():
                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                for i in range(min(16, frame_count)):
                    ret, frame = cap.read()
                    if not ret:
                        break
                cap.release()
            return True
        except:
            return False
    
    def check_disk_performance(self):
        """ãƒ‡ã‚£ã‚¹ã‚¯æ€§èƒ½ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯"""
        print(f"\nğŸ’¾ DISK PERFORMANCE CHECK")
        print("="*50)
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        sample_paths = self.df['dicom_path'].dropna().head(5).tolist()
        
        for path in sample_paths:
            if os.path.exists(path):
                parent_dir = os.path.dirname(path)
                
                # ãƒ‡ã‚£ã‚¹ã‚¯èª­ã¿è¾¼ã¿é€Ÿåº¦ãƒ†ã‚¹ãƒˆ
                print(f"Testing directory: {parent_dir}")
                
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
                    file_size = os.path.getsize(path)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é€Ÿåº¦ï¼ˆRAWï¼‰
                    start_time = time.time()
                    with open(path, 'rb') as f:
                        chunk_size = 1024 * 1024  # 1MB chunks
                        total_read = 0
                        while total_read < file_size:
                            chunk = f.read(min(chunk_size, file_size - total_read))
                            if not chunk:
                                break
                            total_read += len(chunk)
                    
                    read_time = time.time() - start_time
                    throughput_mbps = (file_size / (1024*1024)) / read_time
                    
                    print(f"  File size: {file_size/(1024*1024):.1f}MB")
                    print(f"  Raw read time: {read_time:.3f}s")
                    print(f"  Disk throughput: {throughput_mbps:.1f}MB/s")
                    
                    # æ€§èƒ½è©•ä¾¡
                    if throughput_mbps < 50:
                        print(f"  ğŸš¨ VERY SLOW disk (<50MB/s) - likely HDD or network")
                    elif throughput_mbps < 200:
                        print(f"  âš ï¸  SLOW disk (<200MB/s) - consider SSD upgrade")
                    else:
                        print(f"  âœ… GOOD disk speed (>200MB/s)")
                
                except Exception as e:
                    print(f"  âŒ Error testing disk: {e}")
                
                break  # 1ã¤ã®ãƒ‡ã‚£ã‚¹ã‚¯ã®ã¿ãƒ†ã‚¹ãƒˆ
    
    def generate_optimization_report(self):
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"\nğŸ“‹ OPTIMIZATION RECOMMENDATIONS")
        print("="*50)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ åˆ†æ
        self.analyze_file_system()
        
        # ãƒ“ãƒ‡ã‚ªãƒ­ãƒ¼ãƒ‰æ€§èƒ½ãƒ†ã‚¹ãƒˆ
        video_stats = self.benchmark_single_video_loading(num_videos=5)
        
        # ä¸¦åˆ—åŒ–ãƒ†ã‚¹ãƒˆ
        parallel_speedup = self.test_parallel_loading(num_workers=4, num_videos=8)
        
        # ãƒ‡ã‚£ã‚¹ã‚¯æ€§èƒ½ãƒã‚§ãƒƒã‚¯
        self.check_disk_performance()
        
        # ç·åˆææ¡ˆ
        print(f"\nğŸš€ RECOMMENDED OPTIMIZATIONS")
        print("="*50)
        
        recommendations = []
        
        if video_stats['avg_load_time'] > 5:
            recommendations.extend([
                "ğŸ”¥ CRITICAL: Video loading is extremely slow",
                "   â†’ Move videos to SSD if on HDD",
                "   â†’ Check if videos are on network storage",
                "   â†’ Consider video preprocessing/compression",
                "   â†’ Implement video caching strategy"
            ])
        
        if video_stats['error_rate'] > 0.1:
            recommendations.extend([
                "âš ï¸  High error rate in video loading",
                "   â†’ Check file corruption",
                "   â†’ Verify file permissions",
                "   â†’ Handle missing files gracefully"
            ])
        
        if parallel_speedup < 1.5:
            recommendations.extend([
                "ğŸ“Š Poor parallelization suggests I/O bottleneck",
                "   â†’ Upgrade to faster storage (NVMe SSD)",
                "   â†’ Reduce video resolution/length",
                "   â†’ Implement smart caching"
            ])
        
        if not recommendations:
            recommendations.append("âœ… Video loading performance seems acceptable")
        
        for rec in recommendations:
            print(rec)
        
        return {
            'video_stats': video_stats,
            'parallel_speedup': parallel_speedup,
            'recommendations': recommendations
        }

def run_diagnosis(csv_path):
    """è¨ºæ–­å®Ÿè¡Œ"""
    diagnostics = VideoLoadingDiagnostic(csv_path)
    return diagnostics.generate_optimization_report()

if __name__ == "__main__":
    # ä½¿ç”¨ä¾‹
    dataset_path = "/mnt/s/Workfolder/vector_embedding_echo/dataset/datasplit/"
    csv_path = dataset_path + "train_sel_ds.csv"
    
    print("ğŸ” DIAGNOSING VIDEO LOADING PERFORMANCE")
    print("="*60)
    
    run_diagnosis(csv_path)