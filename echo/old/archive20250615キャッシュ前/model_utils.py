# ä¿®æ­£ã•ã‚ŒãŸmodel_utils.pyï¼ˆæ­£ç¢ºãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚«ã‚¦ãƒ³ãƒˆï¼‰

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Function
import numpy as np

class TrainingConfig:
    def __init__(self, 
                 use_adversarial=True,
                 adversarial_attributes=['Sex', 'Race'],
                 lambda_adv=1.0,
                 dynamic_lambda=True,  # ä¿æŒ
                 use_lora=True,
                 lora_r=8):
        self.use_adversarial = use_adversarial
        self.adversarial_attributes = adversarial_attributes if use_adversarial else []
        self.lambda_adv = lambda_adv
        self.dynamic_lambda = dynamic_lambda  # ä¿æŒ
        self.use_lora = use_lora
        self.lora_r = lora_r

class GradientReversalFunction(Function):
    @staticmethod
    def forward(ctx, x, lambda_):
        ctx.lambda_ = lambda_
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.lambda_ * grad_output, None

class GradientReversalLayer(nn.Module):
    def __init__(self, lambda_=1.0):
        super().__init__()
        self.lambda_ = lambda_

    def forward(self, x):
        return GradientReversalFunction.apply(x, self.lambda_)
    
    def set_lambda(self, lambda_):
        self.lambda_ = lambda_

class AdversarialHead(nn.Module):
    """Adversarial head with GRL"""
    def __init__(self, input_dim=512, num_classes=2, dropout=0.3):
        super().__init__()
        self.grl = GradientReversalLayer()
        self.head = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )

    def forward(self, x):
        x = self.grl(x)
        return self.head(x)
    
    def set_lambda(self, lambda_):
        self.grl.set_lambda(lambda_)


class SimpleLoRA(nn.Module):
    """ä¿®æ­£ã•ã‚ŒãŸLoRAå®Ÿè£…ï¼ˆæœ€ä½é™ã®ä¿®æ­£ï¼‰"""
    def __init__(self, original_layer, r=8, alpha=32, dropout=0.1):
        super().__init__()
        self.original = original_layer
        self.r = r
        self.scaling = alpha / r
        
        # å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ˜ç¤ºçš„ã«å›ºå®š
        for param in self.original.parameters():
            param.requires_grad = False
        
        # LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã“ã‚Œã‚‰ã®ã¿ãŒè¨“ç·´å¯èƒ½ï¼‰
        self.lora_A = nn.Parameter(torch.randn(r, original_layer.in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(original_layer.out_features, r))
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # ğŸš¨ ä¿®æ­£ç®‡æ‰€ï¼šå…ƒã®å‡ºåŠ›ã¯å‹¾é…ãªã—ã§è¨ˆç®—ã€LoRAéƒ¨åˆ†ã®ã¿å‹¾é…ã‚ã‚Š
        with torch.no_grad():
            original_output = self.original(x)
        
        # LoRAéƒ¨åˆ†ï¼ˆå‹¾é…ã‚ã‚Šï¼‰
        lora_output = F.linear(x, (self.lora_B @ self.lora_A) * self.scaling)
        
        # åˆæˆï¼ˆLoRAã®å‹¾é…ãŒä¿æŒã•ã‚Œã‚‹ï¼‰
        return original_output + self.dropout(lora_output)

class EchoEmbeddingModel(nn.Module):
    def __init__(self, base_encoder, config):
        super().__init__()
        self.config = config
        self.base_encoder = base_encoder
        
        # LoRAã‚’é©ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if config.use_lora:
            self._apply_lora()
        
        # Adversarial heads
        self.adversarial_heads = nn.ModuleDict()
        if config.use_adversarial:
            attr_classes = {'Sex': 2, 'Race': 4}
            for attr in config.adversarial_attributes:
                self.adversarial_heads[attr] = AdversarialHead(
                    input_dim=512, 
                    num_classes=attr_classes.get(attr, 2),
                    dropout=0.3
                )
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ±è¨ˆã‚’è¡¨ç¤º
        self._print_trainable_params()
    
    def _apply_lora(self):
        """LoRAã‚’ä¸»è¦ãªLinearå±¤ã«é©ç”¨"""
        target_modules = ['qkv', 'proj']
        
        # æœ€åˆã«å…¨ã¦ã®base_encoderãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å›ºå®š
        for param in self.base_encoder.parameters():
            param.requires_grad = False
        
        lora_applied_count = 0
        for name, module in self.base_encoder.named_modules():
            if isinstance(module, nn.Linear) and any(x in name for x in target_modules):
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                
                if parent_name:
                    parent = dict(self.base_encoder.named_modules())[parent_name]
                    lora_module = SimpleLoRA(
                        module, 
                        r=self.config.lora_r, 
                        alpha=32, 
                        dropout=0.1
                    )
                    setattr(parent, child_name, lora_module)
                    lora_applied_count += 1
        
        print(f"LoRA: adapted to {lora_applied_count} linear layes")
        
        # LoRAé©ç”¨å¾Œã€LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’è¨“ç·´å¯èƒ½ã«ã™ã‚‹
        for name, module in self.base_encoder.named_modules():
            if isinstance(module, SimpleLoRA):
                # LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è¨“ç·´å¯èƒ½
                module.lora_A.requires_grad = True
                module.lora_B.requires_grad = True
                # å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®šã‚’ç¢ºèª
                for param in module.original.parameters():
                    param.requires_grad = False
    
    def _print_trainable_params(self):
        # å…¨ä½“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµ±è¨ˆ
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        print(f"   Total parameters: {total_params:,}")
        print(f"   Trainable parameters: {trainable_params:,}")
        print(f"   Trainable propotion: {100 * trainable_params / total_params:.2f}%")
    
    def forward(self, x):
        features = self.base_encoder(x)
        return features